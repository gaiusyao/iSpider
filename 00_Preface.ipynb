{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 项目背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;作为一名 Python “票友”，利用 Python 进行数据分析已成为了一种爱好，也正尝试着通过 Python 数据分析，让自己的工作和生活变得更有趣。\n",
    "\n",
    "&emsp;&emsp;而在学习 Python 数据分析的过程中，发现优质的外部数据至关重要。之前主要从公开数据集（*一些由科研机构、政府或者企业开放的数据集，一般有着较高的质量*）中获取外部数据。这里由衷感谢 [UCI](http://archive.ics.uci.edu/ml/datasets.html)、[kaggle](https://www.kaggle.com/datasets) 等网站，以及 Google 的 [Dataset Search](https://toolbox.google.com/datasetsearch) ，在它们上面可以找到大量的优质公开数据集，这为获取公开数据集提供了很多的便利。\n",
    "\n",
    "&emsp;&emsp;但有的时候，为了满足对特定目标进行分析的需求，还需要通过爬虫的方式，爬取互联网上的公开信息。笔者之前只写过一些很简单的爬虫，为了满足学习过程中日益增长的数据需求，决心系统地学习一遍 Python 爬虫，并将爬虫的学习笔记记录到 [iSpider](https://github.com/gaiusyao/iSpider) 这个项目中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 创建虚拟环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于 iSpider 是一个新项目，且用到的第三方库较多，为了不影响到其他项目，先搭建一个虚拟环境。这里选择使用 [Anaconda](https://www.anaconda.com/) 自带的 conda 工具创建虚拟环境：\n",
    "```\n",
    "conda create -n iSpider python=3.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 安装第三方库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;为了使用方便，在虚拟环境创建完成后，先安装一些常用的第三方库。以笔者浅薄鄙陋的经验，这一步主要安装了下列第三方库及其依赖（*其他的之后用到再说*）：\n",
    "- 数据科学常用库：\n",
    "    - NumPy：最基础的包，提供了多种数据结构、算法及数值计算所需的接口；\n",
    "    - Pandas：提供了高级数据结构和函数，主要用于数据操作、预处理和清洗；\n",
    "    - Scipy：科学计算领域中针对不同标准问题域的包集合；\n",
    "    - statsmodels：统计分析包；\n",
    "    - scikit-learn：机器学习工具包。\n",
    "- 爬虫框架：\n",
    "    - Scrapy：一个为了爬取网站数据，提取结构性数据而编写的应用框架。\n",
    "- 请求库：\n",
    "    - requests：用于模拟浏览器向服务器发送请求；\n",
    "    - selenium：一个自动化测试工具，可以用于驱动浏览器执行特定动作。\n",
    "- 解析库：\n",
    "    - lxml：解析 HTML 或 XML，支持 XPath；\n",
    "    - beautifulsoup4：依赖于 lxml，解析 HTML 或 XML，拥有强大的 API 和多样的解析方式；\n",
    "    - pyquery：提供了和 jQuery 类似的语法来解析 HTML。\n",
    "- 可视化库：\n",
    "    - matplotlib：一个 Python 2D 绘图库，可以在各种平台上生成具有出版品质的图形；\n",
    "    - seaborn：依赖于 matplotlib，提供更好的可视化效果；\n",
    "    - pyecharts：基于Echarts.js 的可视化图表库。\n",
    "- 其他库：\n",
    "    - jieba：用于中文分词；\n",
    "    - PyMySQL：连接 MySQL 数据库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上述库，除了 `Scrapy` 是使用 `conda install Scrapy` 安装（*依赖较多*）的以外，其他都是采用 `pip install packageName` 的形式安装的。\n",
    "\n",
    "&emsp;&emsp;若嫌一个个安装较麻烦，可以通过 [iSpider\\requirements.txt](https://github.com/gaiusyao/iSpider/blob/master/requirements.txt) 文件快速安装相关的第三方库。\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "&emsp;&emsp;生成自己的 `requirements.txt` 文件也非常容易，只需在命令行输入：\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 学习计划"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Python 爬虫入门的学习过程大致分为 12 个阶段，预计耗时 12 周（没摸鱼的理想状态下），每阶段的学习笔记都会以 `ipynb` 的形式记录下来，并同步到 [iSpider](https://github.com/gaiusyao/iSpider) 上。\n",
    "\n",
    "> **TODO List:**\n",
    "1. 爬虫基础\n",
    "2. 发送请求\n",
    "3. 数据解析\n",
    "4. 数据存储\n",
    "5. Ajax 数据爬取\n",
    "6. 动态渲染页面爬取\n",
    "7. 使用代理\n",
    "8. 模拟登录\n",
    "9. APP 数据的抓取\n",
    "10. Scrapy 框架的使用\n",
    "11. 分布式爬虫\n",
    "12. 反反爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
